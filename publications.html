
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Publications</title>
  <meta name="description" content="Homepage for Lanzhe Guo.">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" id="bulma" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.6.0/css/bulma.min.css" />
  <link rel="stylesheet" type="text/css" href="/styles/base.css">
  <link rel="stylesheet" type="text/css" href="/styles/academicons.min.css">

  <link rel="canonical" href="http://www.guolz.com/publications">
</head>

  <body>
    <section class="hero is-fullheight">
      <div class="hero-head">
        <nav class="navbar" role="navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="/">
      <strong>Lanzhe Guo (郭 兰哲)</strong>
    </a>

    <div class="navbar-burger" data-target="navbar-main">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>

  <div class="navbar-menu" id="navbar-main">
    <div class="navbar-start">
      <!-- navbar items -->
      <a href="/" class="navbar-item">
        About
      </a>
      <a class="navbar-item" href="/publications">
        Publications & Pre
      </a>
      <a class="navbar-item" href="/resume.pdf">
        Resume
      </a>
      <a class="navbar-item" href="/share">
        Sharing
      </a>
    </div>

    <div class="navbar-end">
        <!-- navbar items -->
        <div class="navbar-item">
          <a href="mailto:guolz@lamda.nju.edu.cn" class="button is-white">
            <i class="fa fa-lg fa-envelope-o" aria-hidden="true"></i>
          </a>
          <a href="https://github.com/JLUNeverMore" class="button is-white">
            <i class="fa fa-lg fa-github" aria-hidden="true"></i>
          </a>
          <a href="https://www.zhihu.com/people/gzxl" class="button is-white">
              <img src="images/zhihu.png" width="20" height="20">
          </a>
          <!--<a href="https://www.douban.com/people/pluskid/" class="button is-white">
            <img src="/images/douban.png" width="20" height="20">
          </a>-->
        </div>
      </div>
    </div>
  </nav>

      </div>

      <div class="hero-body">
        <div class="container">
  <article class="media">
    <div class="media-content">
      <div class="content">
        <h1>Publications</h1>
      </div>
    </div>
  </article>

  <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        <b>Lan-Zhe Guo</b>, Yu-Feng Li. <em>A General Formulation for Safely Exploiting Weakly Supervised Data</em>.
        In: Proceedings of the 32nd AAAI conference on Artificial Intelligence (AAAI'18), New Orleans, LA.
        <a class="tag" href="/pdf/aaai18.pdf">PDF</a>
        </p>
        <p>
          Weakly supervised data is an important machine learning data
to help improve learning performance. However, recent results
indicate that machine learning techniques with the usage
of weakly supervised data may sometimes cause performance
degradation. Safely leveraging weakly supervised data is important,
whereas there is only very limited effort, especially
on a general formulation to help provide insight to guide
safe weakly supervised learning. In this paper we present a
scheme that builds the final prediction results by integrating
several weakly supervised learners. Our resultant formulation
brings two advantages. i) For the commonly used convex loss
functions in both regression and classification tasks, safeness
guarantees exist under a mild condition; ii) Prior knowledge
related to the weights of base learners can be embedded in a
flexible manner. Moreover, the formulation can be addressed
globally by simple convex quadratic or linear program efficiently.
Experiments on multiple weakly supervised learning
tasks such as label noise learning, domain adaptation and
semi-supervised learning validate the effectiveness.
        </p>
      </div>
    </div>
  </article>
          
  <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        <b>Lan-Zhe Guo</b>, Shao-Bo Wang, Yu-Feng Li. <em>Large Margin Graph Construction for Semi-Supervised Learning</em>.
        In: 2018 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE, 2018: 1030-1033.
        <a class="tag" href="/pdf/ICDMW18.pdf">PDF</a>
        </p>
        <p>
        Graph-based semi-supervised learning (GSSL) has gained increased interests in the last few years. 
       A large number of empirical results show that the performance of GSSL methods heavily depends on the graph 
       construction approach. Although great efforts have been devoted to construct good graphs, 
       it remains challenging to construct a good graph in general situations.
       To alleviate this problem, this paper presents a novel graph construction approach. 
        Unlike previous approaches that typically optimize a kNN-type loss on the unlabeled data, 
         the proposed approach further enforces that the prediction of unlabeled data has a large margin separation 
         so as to help exclude low-quality graphs. We formulate the problem as an optimization and present 
        an efficient algorithm. Experimental results on benchmark data sets show that the proposed approach
        has a stronger ability to construct good graphs comparing with several representative graph construction approaches.
        </p>
      </div>
    </div>
  </article>        
         
            <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        <b>Lan-Zhe Guo</b>, Tao Han, Yu-Feng Li. <em>Robust Semi-Supervised Representation Learning for Graph-Structured Data</em>.
        In: Proceedings of the 23rd Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD'19). Macau, China. 2019.
        <a class="tag" href="/pdf/PAKDD19.pdf">PDF</a>
        </p>
        <p>
       The success of machine learning algorithms generally depends on data representation and recently many representation learning
methods have been proposed. However, learning a good representation
may not always benefit the classification tasks. It sometimes even hurt
the performance as the learned representation maybe not related to the
ultimate tasks, especially when the labeled examples are few to afford a
reliable model selection. In this paper, we propose a novel robust semisupervised graph representation learning method based on graph convolutional network. To make the learned representation more related to
the ultimate classification task, we propose to extend label information
based on the smooth assumption and obtain pseudo-labels for unlabeled
nodes. Moreover, to make the model robust with noise in the pseudolabel, we propose to apply a large margin classifier to the learned representation. Influenced by the pseudo-label and the large-margin principle,
the learned representation can not only exploit the label information encoded in the graph-structure sufficiently but also can produce a more
rigorous decision boundary. Experiments demonstrate the superior performance of the proposal over many related methods.
        </p>
      </div>
    </div>
  </article>  
          
     <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        <b>Lan-Zhe Guo</b>, Yu-Feng Li, Ming Li, Jin-Feng Yi, Bo-Wen Zhou, Zhi-Hua Zhou. <em>Reliable Weakly Supervised Learning: Maximize Gain and Maintain Safeness</em>.
        arXiv preprint arXiv:1904.09743
        <a class="tag" href="/pdf/arxiv.pdf">PDF</a>
        </p>
        <p>
       Weakly supervised data are widespread and have attracted much attention. However, since label quality is often difficult to guarantee, sometimes the use of weakly supervised data will lead to unsatisfactory performance, ie, performance degradation or poor performance gains. Moreover, it is usually not feasible to manually increase the label quality, which results in weakly supervised learning being somewhat difficult to rely on. In view of this crucial issue, this paper proposes a simple and novel weakly supervised learning framework. We guide the optimization of label quality through a small amount of validation data, and to ensure the safeness of performance while maximizing performance gain. As validation set is a good approximation for describing generalization risk, it can effectively avoid the unsatisfactory performance caused by incorrect data distribution assumptions. We formalize this underlying consideration into a novel Bi-Level optimization and give an effective solution. Extensive experimental results verify that the new framework achieves impressive performance on weakly supervised learning with a small amount of validation data.
        </p>
      </div>
    </div>
  </article>  
          
  <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
        Tong Wei, <b>Lan-Zhe Guo</b>, Yu-Feng Li, Wei Gao. <em>Learning Safe Multi-Label Prediction for Weakly Labeled Data</em>. 
        Machine Learning, 2018, 107(4): 703-725.
        <a class="tag" href="/pdf/acml17.pdf">PDF</a>
        </p>
        <p>
          In this paper we study multi-label learning with weakly labeled data,
i.e., labels of training examples are incomplete. This includes, e.g., (i) semi-supervised
multi-label learning where completely labeled examples are partially known; (ii)
weak label learning where relevant labels of examples are partially known; iii) extended
weak label learning where relevant and irrelevant labels of examples are
partially known. Weakly labeled data commonly occur in real applications, e.g.,
image classification, document categorization. Previous studies often expect that
learning methods with the use of weakly labeled data improve learning performance,
as more data are employed. This, however, is not always the cases in reality.
Using more weakly labeled data may sometimes degenerate learning performance.
It is desirable to learn safe multi-label prediction that will not hurt performance
when weakly labeled data is used. In this work we optimize multi-label evaluation
metrics (F1 score and Top-k precision) given that ground-truth label assignments
are realized by a convex combination of basic multi-label learners. To cope with
infinite number of possible ground-truth label assignments, cutting-plane strategy
is adopted to iteratively generate the most helpful label assignments. The
whole optimization is cast as a series of simple linear programs in an efficient
manner. Extensive experiments on three weakly labeled learning tasks, namely,
i) semi-supervised multi-label learning; ii) weak-label learning and iii) extended
weak-label learning, show that our proposal clearly improves the safeness in comparison
to many state-of-the-art methods.
        </p>
      </div>
    </div>
</div>


</div>

<div class="hero-body">
  <div class="container">
<article class="media">
<div class="media-content">
<div class="content">
  <h1>Presentations</h1>
  <article class="media">
    <div class="media-content">
      <div class="content">
        <p>
            InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. In LAMDA-12 group meeting, June.06 2018.
            <a class = "tag", href = "https://arxiv.org/abs/1606.03657">PDF</a>
            <a class = "tag", href = /pdf/infogan.pdf>Slides</a>.
        </p>
      </div>
    </div>
  </article>
  <article class="media">
      <div class="media-content">
        <div class="content">
          <p>
               Semi-Supervised Classification with Graph Convolutional Networks, In LAMDA-12 group meeting, April.04 2018.
              <a class = "tag", href = "https://arxiv.org/abs/1609.02907">PDF</a>
              <a class = "tag", href = /pdf/gcn.pdf>Slides</a>.
          </p>
        </div>
      </div>
    </article>
    <article class="media">
      <div class="media-content">
        <div class="content">
          <p>
              A General Formulation for Safely Exploiting Weakly Supervised Data, In AAAI-18, New Orleans.
              <a class="tag" href="/pdf/aaai18.pdf">PDF</a>
              <a class = "tag", href = /pdf/safew.pdf>Slides</a>.
          </p>
        </div>
      </div>
    </article>
</div>
</div>
</article>
</div>

      </div>
      <div class="hero-foot">
        <footer class="footer">
  <div class="container">
    <div class="content has-text-right is-size-7">
      <p>
        © 2018 by Lan-Zhe Guo. All Rights Reserved.
        &nbsp;&nbsp;
          Links:&nbsp;&nbsp;
          <a target="www.sshao.com" href="#">Shuai Shao</a>
      </p>
    </div>
  </div>
</footer>

<script async type="text/javascript" src="/javascript/bulma.js"></script>

      </div>
    </section>
  </body>
</html>
